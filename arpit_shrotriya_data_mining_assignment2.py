# -*- coding: utf-8 -*-
"""Arpit_Shrotriya_Data_Mining_Assignment2_mk5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1utQOhywHDDSN9yom3NDBed8kGmndSrt7
"""

import time
import itertools
import pandas as pd
from collections import defaultdict

print("##########################################################################################################")
print("Welcome!!!!!!!!!!")
print("---------------------------------------------------------------------------------------------------------")
n = int(input("\nSelect the Supermarket: \n1.Amazon \n2.K-Mart \n3.Shoprite \n4.Target \n5.Walmart \n[1-5]"))
min_support = float(input("\nEnter Minimum Support (Enter Float Value Between 0-1)"))
min_confidence = float(input("\nEnter Minimum Confidence (Enter Float Value Between 0-1)"))
if (n==1):
  df = pd.read_csv("Amazon_transactions_final.csv")
elif (n==2):
  df = pd.read_csv("K-Mart_transactions_final.csv")
elif (n==3):
  df = pd.read_csv("Shoprite_transactions_final.csv")
elif (n==4):
  df = pd.read_csv("Target_transactions_final.csv")
elif (n==5):
  df = pd.read_csv("Walmart_transactions_final.csv")

transactions = df['Transaction'].apply(lambda x: x.split(', ')).tolist()
all_items = sorted(set(item for transaction in transactions for item in transaction))
#print(transactions)
#print(len(transactions))
#print(all_items)

# Brute Force Method

def calculate_support(itemset, transactions):
  count = 0
  for transaction in transactions:
    if set(itemset).issubset(set(transaction)):
      count += 1
  return count / len(transactions)



def find_frequent_itemsets(transactions, all_items, min_support):
  k = 1
  frequent_itemsets = []
  candidate_itemsets = all_items

  while candidate_itemsets:
    print(f'Generating {k}-itemsets')
    current_frequent_itemsets = []
    if k==1:
      candidate_itemsets = [[item] for item in candidate_itemsets]
    else:
      candidate_itemsets = [list(itemset) for itemset in itertools.combinations(all_items, k)]

    for itemset in candidate_itemsets:
      support = calculate_support(itemset, transactions)
      if support >= min_support:
        current_frequent_itemsets.append((itemset, support))

    if not current_frequent_itemsets:
      break

    frequent_itemsets.extend(current_frequent_itemsets)
    k += 1

  return frequent_itemsets

def generate_association_rules(frequent_itemsets, transactions, min_confidence):
  rules = []
  for itemset, support in frequent_itemsets:
    if len(itemset) < 2:
      continue
    for i in range(1, len(itemset)):
      for antecedent in itertools.combinations(itemset, i):
        consequent = list(set(itemset) - set(antecedent))
        antecedent_support = calculate_support(antecedent, transactions)
        if antecedent_support > 0:
          confidence = support / antecedent_support
          if confidence >= min_confidence:
            rules.append((antecedent, consequent, confidence))

  return rules

# Brute Force Method

start_time = time.perf_counter()
frequent_itemsets = find_frequent_itemsets(transactions, all_items, min_support)
end_time = time.perf_counter()
brute_force_time = end_time - start_time


#print(f"Brute Force Time : {brute_force_time}")
frequent_itemsets_dict = {tuple(itemset): support for itemset, support in frequent_itemsets}
#print(frequent_itemsets)
print("\nFrequent Itemsets (Brute Force):")
for itemset, support in frequent_itemsets:
  print(f"Itemset: {itemset}, Support: {support}")

# Generating Association Rules for Brute Force
association_rules_brute_force = generate_association_rules(frequent_itemsets, transactions, min_confidence)
association_rules_brute_force_set = set((frozenset(antecedent), frozenset(consequent), confidence) for antecedent, consequent, confidence in association_rules_brute_force)

print("\nAssociation Rules (Brute Force): ")
for antecedent, consequent, confidence in association_rules_brute_force_set:
  print(f"Rule: {antecedent} -> {consequent}, confidence: {confidence:.2f}")

from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder
import pandas as pd
import time

# Assuming transactions and min_support, min_confidence are defined earlier in the code
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_encoded = pd.DataFrame(te_ary, columns=te.columns_)

# Run Apriori Algorithm
start_time = time.perf_counter()
frequent_itemsets_apriori = apriori(df_encoded, min_support=min_support, use_colnames=True)
end_time = time.perf_counter()
apriori_time = end_time - start_time


#print(f"\nApriori Time : {apriori_time}")

# Check if any frequent itemsets were found
if frequent_itemsets_apriori.empty:
    print("No frequent itemsets were found. Exiting.")
else:
    print("\nFrequent Itemsets (Apriori):")
    print(frequent_itemsets_apriori)

    # Generate association rules if frequent itemsets exist
    association_rules_apriori = association_rules(frequent_itemsets_apriori, metric="confidence", min_threshold=min_confidence)
    association_rules_apriori_set = set(
        (frozenset(antecedent), frozenset(consequent), confidence)
        for antecedent, consequent, confidence in zip(
            association_rules_apriori['antecedents'],
            association_rules_apriori['consequents'],
            association_rules_apriori['confidence']
        )
    )
    print("\nAssociation Rules (Apriori):")
    print(association_rules_apriori[['antecedents', 'consequents', 'confidence']])

from mlxtend.frequent_patterns import fpgrowth, association_rules
import pandas as pd
import time

# Assuming df_encoded, min_support, and min_confidence are defined earlier in the code

# Run FP-Growth Algorithm
start_time = time.perf_counter()
frequent_itemsets_fp = fpgrowth(df_encoded, min_support=min_support, use_colnames=True)
end_time = time.perf_counter()
fp_growth_time = end_time - start_time


#print(f"\nFP-Growth Time : {fp_growth_time}")

# Check if any frequent itemsets were found
if frequent_itemsets_fp.empty:
    print("No frequent itemsets were found. Exiting.")
else:
    print("\nFrequent Itemsets (FP-Growth):")
    print(frequent_itemsets_fp)

    # Create dictionary of frequent itemsets
    frequent_itemsets_fp_dict = {tuple(row['itemsets']): row['support'] for _, row in frequent_itemsets_fp.iterrows()}

    # Generate association rules if frequent itemsets exist
    association_rules_fp_growth = association_rules(frequent_itemsets_fp, metric='confidence', min_threshold=min_confidence)
    association_rules_fp_growth_set = set(
        (frozenset(antecedents), frozenset(consequents), confidence)
        for antecedents, consequents, confidence in zip(
            association_rules_fp_growth['antecedents'],
            association_rules_fp_growth['consequents'],
            association_rules_fp_growth['confidence']
        )
    )
    print("\nAssociation Rules (FP-Growth):")
    print(association_rules_fp_growth[['antecedents', 'consequents', 'confidence']])

# Compare association rules between brute force, apriori, and fp-growth
print('\nIntersection of Association Rules (Brute Force vs Apriori):')
common_rules_bf_apriori = association_rules_brute_force_set.intersection(association_rules_apriori_set)
for rule in common_rules_bf_apriori:
    print(f'Rule: {set(rule[0])} -> {set(rule[1])}, Confidence: {rule[2]:.2f}')

print('\nIntersection of Association Rules (Brute Force vs FP-Growth):')
common_rules_bf_fp = association_rules_brute_force_set.intersection(association_rules_fp_growth_set)
for rule in common_rules_bf_fp:
    print(f'Rule: {set(rule[0])} -> {set(rule[1])}, Confidence: {rule[2]:.2f}')

print('\nIntersection of Association Rules (Apriori vs FP-Growth):')
common_rules_apriori_fp = association_rules_apriori_set.intersection(association_rules_fp_growth_set)
for rule in common_rules_apriori_fp:
    print(f'Rule: {set(rule[0])} -> {set(rule[1])}, Confidence: {rule[2]:.2f}')

print('\nTiming Performance:')
print(f'Brute Force Time: {brute_force_time:.4f} seconds')
print(f'Apriori Time: {apriori_time:.4f} seconds')
print(f'FP-Growth Time: {fp_growth_time:.4f} seconds')